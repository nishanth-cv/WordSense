{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "517d79a5-8d23-49bc-a332-ef7c082e2fff",
   "metadata": {},
   "source": [
    "### Environment Setup and Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00140217-196d-476f-819c-aa2fa128566a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /home/chockalingamveerap.n/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /home/chockalingamveerap.n/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/chockalingamveerap.n/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/chockalingamveerap.n/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/chockalingamveerap.n/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Environment Setup and Package Imports\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab1ef488-593c-4e86-8447-36f585069f34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Utility functions for text cleaning and caching\n",
    "\n",
    "def clean_text(text, remove_punctuation=False):\n",
    "    # Convert to lowercase and remove newlines and extra spaces\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    if remove_punctuation:\n",
    "        # Remove all punctuation if specified\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text.strip()\n",
    "\n",
    "def load_or_cache(filename, loader_func, save_format='json'):\n",
    "    \"\"\"\n",
    "    Checks if cached file exists. If so, loads it.\n",
    "    Otherwise, calls `loader_func` to generate data, caches it, and returns it.\n",
    "    Supports both JSON and pickle formats.\n",
    "    \"\"\"\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"Loading from cache: {filename}\")\n",
    "        with open(filename, 'r' if save_format == 'json' else 'rb') as f:\n",
    "            return json.load(f) if save_format == 'json' else pickle.load(f)\n",
    "    else:\n",
    "        print(f\"Processing and caching: {filename}\")\n",
    "        data = loader_func()\n",
    "        with open(filename, 'w' if save_format == 'json' else 'wb') as f:\n",
    "            (json.dump(data, f) if save_format == 'json' else pickle.dump(data, f))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4ac1813-9aa5-4866-abad-b7336fc3d70e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from cache: gutenberg_cleaned.json\n",
      "Loading from cache: brown_cleaned.json\n",
      "Loading from cache: wikitext_cleaned.pkl\n"
     ]
    }
   ],
   "source": [
    "# Load NLTK and HuggingFace datasets\n",
    "\n",
    "def load_gutenberg_corpus():\n",
    "    from nltk.corpus import gutenberg\n",
    "    return {fid: clean_text(gutenberg.raw(fid)) for fid in gutenberg.fileids()}\n",
    "\n",
    "def load_brown_corpus():\n",
    "    from nltk.corpus import brown\n",
    "    return {\n",
    "        cat: clean_text(\" \".join(brown.words(categories=cat)))\n",
    "        for cat in brown.categories()\n",
    "    }\n",
    "\n",
    "def load_wikitext():\n",
    "    dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"train\")\n",
    "    return [clean_text(example['text']) for example in dataset if example['text'].strip()]\n",
    "\n",
    "# Load with caching\n",
    "gutenberg_texts = load_or_cache(\"gutenberg_cleaned.json\", load_gutenberg_corpus)\n",
    "brown_texts = load_or_cache(\"brown_cleaned.json\", load_brown_corpus)\n",
    "wikitext_texts = load_or_cache(\"wikitext_cleaned.pkl\", load_wikitext, save_format='pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbb0c177-fe80-4f85-ac0d-9b00e2e1768e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gutenberg Sample:\n",
      "File: austen-emma.txt\n",
      "[emma by jane austen 1816] volume i chapter i emma woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to unite some of the best blessings of existence; and had lived nearly twenty-one years in the world with very little to distress or vex her. she was the youngest of the two daughters of a most affectionate, indulgent father; and had, in consequence of her sister's marriage, been mistress of his house from a very early period. her mother had died too lon\n",
      "\n",
      "Brown Corpus Sample:\n",
      "Category: adventure\n",
      "dan morgan told himself he would forget ann turner . he was well rid of her . he certainly didn't want a wife who was fickle as ann . if he had married her , he'd have been asking for trouble . but all of this was rationalization . sometimes he woke up in the middle of the night thinking of ann , and then could not get back to sleep . his plans and dreams had revolved around her so much and for so long that now he felt as if he had nothing . the easiest thing would be to sell out to al budd and \n",
      "\n",
      "WikiText-103 Sample:\n",
      "senjō no valkyria 3 : unrecorded chronicles ( japanese : 戦場のヴァルキュリア3 , lit . valkyria of the battlefield 3 ) , commonly referred to as valkyria chronicles iii outside japan , is a tactical role @-@ playing video game developed by sega and media.vision for the playstation portable . released in january 2011 in japan , it is the third game in the valkyria series . employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and fo\n"
     ]
    }
   ],
   "source": [
    "# Preview sample outputs from each corpus\n",
    "\n",
    "print(\"Gutenberg Sample:\")\n",
    "for fileid, text in list(gutenberg_texts.items())[:1]:\n",
    "    print(f\"File: {fileid}\")\n",
    "    print(text[:500])\n",
    "    break\n",
    "\n",
    "print(\"\\nBrown Corpus Sample:\")\n",
    "for category, text in list(brown_texts.items())[:1]:\n",
    "    print(f\"Category: {category}\")\n",
    "    print(text[:500])\n",
    "    break\n",
    "\n",
    "print(\"\\nWikiText-103 Sample:\")\n",
    "print(wikitext_texts[0][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab887142-2554-44e4-82af-de926136ea3d",
   "metadata": {},
   "source": [
    "### Annotation Using GPT-3.5 Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4724b5dd-1473-4e2c-86a1-b7262aaa093c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load OpenAI API key securely from .env file\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import openai\n",
    "import os\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7cfae20-9c93-40bc-8b8c-7cac968c46ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful!\n",
      "Response: Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# Verify OpenAI connection is working\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def check_connection():\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Connection test.\"},\n",
    "                {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "            ],\n",
    "            max_tokens=10\n",
    "        )\n",
    "        print(\"Connection successful!\")\n",
    "        print(\"Response:\", response.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        print(\"Connection failed:\", e)\n",
    "\n",
    "check_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7077a722-5b53-40c1-aa34-fe5e148f5d93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Annotate passages using GPT-3.5 Turbo with original prompt format\n",
    "\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "def clean_json_response(content):\n",
    "    \"\"\"\n",
    "    Cleans model output by removing markdown formatting like ```json ... ```\n",
    "    \"\"\"\n",
    "    content = content.strip()\n",
    "    if content.startswith(\"```json\"):\n",
    "        content = content[7:]\n",
    "    if content.endswith(\"```\"):\n",
    "        content = content[:-3]\n",
    "    return content.strip()\n",
    "\n",
    "def annotate_passage(passage, max_words=100, retry_count=3):\n",
    "    \"\"\"\n",
    "    Annotates a passage using the OpenAI API (GPT-3.5 Turbo).\n",
    "    Identifies up to 10 contextually difficult words with definitions.\n",
    "    Includes retry logic and passage splitting if too long.\n",
    "    \"\"\"\n",
    "    if len(passage.split()) < 8:\n",
    "        print(\"Passage too short; skipping.\")\n",
    "        return {\"annotations\": []}\n",
    "\n",
    "    prompt = (\n",
    "        'You are an expert in language comprehension. Given a passage, identify up to 10 contextually difficult or '\n",
    "        'uncommon words (those that might be challenging at sight for an adult) and provide clear, concise definitions '\n",
    "        'for each. Structure your response in JSON format using the following template:\\n\\n'\n",
    "        '{\"annotations\": [{\"word\": \"example\", \"definition\": \"concise definition\"}]}\\n\\n'\n",
    "        \"Passage:\\n\" + passage\n",
    "    )\n",
    "\n",
    "    for attempt in range(retry_count):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a language expert that provides clear and concise definitions.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.3,\n",
    "                max_tokens=400\n",
    "            )\n",
    "            content = clean_json_response(response.choices[0].message.content)\n",
    "            return json.loads(content)\n",
    "\n",
    "        except Exception as e:\n",
    "            if \"context\" in str(e).lower() or \"tokens\" in str(e).lower():\n",
    "                # If passage is too long, split and try again with smaller segments\n",
    "                print(\"Passage too long. Splitting and retrying...\")\n",
    "                if max_words <= 20:\n",
    "                    return {\"annotations\": []}\n",
    "                segments = [\" \".join(passage.split()[i:i + max_words]) for i in range(0, len(passage.split()), max_words)]\n",
    "                combined = {\"annotations\": []}\n",
    "                for seg in segments:\n",
    "                    ann = annotate_passage(seg, max_words=max_words // 2, retry_count=retry_count)\n",
    "                    combined[\"annotations\"].extend(ann.get(\"annotations\", []))\n",
    "                return combined\n",
    "\n",
    "            elif \"rate limit\" in str(e).lower() or \"connection\" in str(e).lower():\n",
    "                # Retry on transient issues like rate limit or network errors\n",
    "                wait = 5 * (attempt + 1)\n",
    "                print(f\"Retrying after {wait} seconds...\")\n",
    "                time.sleep(wait)\n",
    "            else:\n",
    "                # Log any unexpected error and exit\n",
    "                print(\"Unexpected error:\", e)\n",
    "                traceback.print_exc()\n",
    "                return {\"annotations\": []}\n",
    "\n",
    "    print(\"Failed after retries for:\", passage[:100])\n",
    "    return {\"annotations\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55f14423-02c2-475b-954f-0d4fc3809919",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Passage Analysis:\n",
      "- Number of sentences: 7448\n",
      "- Number of words:     158167\n",
      "\n",
      "First 5 sentences:\n",
      "\n",
      ". [emma by jane austen 1816] volume i chapter i emma woodhouse, handsome, clever, and rich, with a comfortable home and happy disposition, seemed to unite some of the best blessings of existence; and had lived nearly twenty-one years in the world with very little to distress or vex her.\n",
      ". she was the youngest of the two daughters of a most affectionate, indulgent father; and had, in consequence of her sister's marriage, been mistress of his house from a very early period.\n",
      ". her mother had died too long ago for her to have more than an indistinct remembrance of her caresses; and her place had been supplied by an excellent woman as governess, who had fallen little short of a mother in affection.\n",
      ". sixteen years had miss taylor been in mr. woodhouse's family, less as a governess than a friend, very fond of both daughters, but particularly of emma.\n",
      ". between _them_ it was more the intimacy of sisters.\n"
     ]
    }
   ],
   "source": [
    "# Pick the first passage and inspect its structure\n",
    "first_passage = list(gutenberg_texts.values())[0]  # or brown_texts / wikitext_texts if you prefer\n",
    "\n",
    "# Tokenize into sentences\n",
    "sentences     = nltk.sent_tokenize(first_passage)\n",
    "num_sentences = len(sentences)\n",
    "num_words     = len(first_passage.split())\n",
    "\n",
    "print(\"First Passage Analysis:\")\n",
    "print(f\"- Number of sentences: {num_sentences}\")\n",
    "print(f\"- Number of words:     {num_words}\")\n",
    "print(\"\\nFirst 5 sentences:\\n\")\n",
    "for sentence in sentences[:5]:\n",
    "    print(\".\", sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "125581db-1ccf-42de-8d84-be2539111678",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate dataset by sampling and annotating passages\n",
    "\n",
    "import random\n",
    "import nltk\n",
    "\n",
    "NUM_PASSAGES_TO_ANNOTATE = 2000    # Total passages to draw from all sources\n",
    "SAMPLES_PER_PASSAGE      = 3       # Number of samples to generate per passage\n",
    "NUM_SENTENCES_PER_SAMPLE = 5       # Number of contiguous sentences in each sample\n",
    "\n",
    "def sample_contiguous_sentences(passage, num_sentences=NUM_SENTENCES_PER_SAMPLE):\n",
    "    \"\"\"\n",
    "    Randomly samples a span of contiguous sentences from a passage.\n",
    "    Falls back to full passage if too short.\n",
    "    \"\"\"\n",
    "    sentences = nltk.sent_tokenize(passage)\n",
    "    if len(sentences) <= num_sentences:\n",
    "        return \" \".join(sentences)\n",
    "    start = random.randint(0, len(sentences) - num_sentences)\n",
    "    return \" \".join(sentences[start:start + num_sentences])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2b21e2b-5373-4f67-b6a5-b5d974a51190",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading curated dataset from file: curated_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# Annotate or load dataset with difficult word definitions\n",
    "\n",
    "dataset_path = \"curated_dataset.csv\"\n",
    "\n",
    "if os.path.exists(dataset_path):\n",
    "    print(\"Loading curated dataset from file: \"+ dataset_path)\n",
    "    annotations_df = pd.read_csv(dataset_path)  # Load from disk if it already exists\n",
    "else:\n",
    "    print(\"Annotating new dataset...\")\n",
    "\n",
    "    # Combine passages from all three sources and shuffle\n",
    "    all_passages = list(gutenberg_texts.values()) + list(brown_texts.values()) + wikitext_texts\n",
    "    random.shuffle(all_passages)\n",
    "\n",
    "    annotations_list = []\n",
    "\n",
    "    # Iterate through a fixed number of passages\n",
    "    for i, passage in enumerate(all_passages[:NUM_PASSAGES_TO_ANNOTATE]):\n",
    "        print(f\"Annotating passage {i+1}/{NUM_PASSAGES_TO_ANNOTATE}\")\n",
    "        for _ in range(SAMPLES_PER_PASSAGE):\n",
    "            sample_text = sample_contiguous_sentences(passage)       # Sample 5 contiguous sentences\n",
    "            annotation = annotate_passage(sample_text)               # Use GPT to annotate\n",
    "            for item in annotation.get(\"annotations\", []):\n",
    "                annotations_list.append({\n",
    "                    \"passage\": sample_text,\n",
    "                    \"word\": item.get(\"word\", \"\").strip(),\n",
    "                    \"definition\": item.get(\"definition\", \"\").strip()\n",
    "                })\n",
    "\n",
    "    # Save the dataset to CSV\n",
    "    annotations_df = pd.DataFrame(annotations_list)\n",
    "    annotations_df.to_csv(dataset_path, index=False)\n",
    "    print(f\"Saved to {dataset_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e75e1f-0bc8-4747-80ab-a77371033d39",
   "metadata": {},
   "source": [
    "### Prepare Dataset for Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec6edfee-e09a-4c79-b060-a4daa67d3fe1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passage</th>\n",
       "      <th>word</th>\n",
       "      <th>definition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the brigade contains four subordinate battalio...</td>\n",
       "      <td>subordinate</td>\n",
       "      <td>lower in rank or position</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the brigade contains four subordinate battalio...</td>\n",
       "      <td>battalions</td>\n",
       "      <td>large organized groups of soldiers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the brigade contains four subordinate battalio...</td>\n",
       "      <td>headquarters</td>\n",
       "      <td>central office or command center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the brigade contains four subordinate battalio...</td>\n",
       "      <td>company</td>\n",
       "      <td>a unit of soldiers forming part of a battalion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the brigade contains four subordinate battalio...</td>\n",
       "      <td>division</td>\n",
       "      <td>a large military unit typically consisting of ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             passage          word  \\\n",
       "0  the brigade contains four subordinate battalio...   subordinate   \n",
       "1  the brigade contains four subordinate battalio...    battalions   \n",
       "2  the brigade contains four subordinate battalio...  headquarters   \n",
       "3  the brigade contains four subordinate battalio...       company   \n",
       "4  the brigade contains four subordinate battalio...      division   \n",
       "\n",
       "                                          definition  \n",
       "0                          lower in rank or position  \n",
       "1                 large organized groups of soldiers  \n",
       "2                   central office or command center  \n",
       "3     a unit of soldiers forming part of a battalion  \n",
       "4  a large military unit typically consisting of ...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and clean the annotated dataset\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"curated_dataset.csv\")\n",
    "\n",
    "# Drop rows with any missing values\n",
    "df.dropna(subset=[\"passage\", \"word\", \"definition\"], inplace=True)\n",
    "\n",
    "# Show sample\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31174fbb-fd86-4869-8689-8a2fc049234f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Text: the brigade contains four subordinate ba...\n",
       "1    Text: the brigade contains four subordinate ba...\n",
       "2    Text: the brigade contains four subordinate ba...\n",
       "3    Text: the brigade contains four subordinate ba...\n",
       "4    Text: the brigade contains four subordinate ba...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert dataset to instruction-response format for fine-tuning\n",
    "\n",
    "def build_prompt(row):\n",
    "    return f\"Text: {row['passage']}\\nQ: Identify difficult words and define them.\\nA: {row['word']} — {row['definition']}\"\n",
    "\n",
    "df[\"text\"] = df.apply(build_prompt, axis=1)\n",
    "\n",
    "# Preview formatted rows\n",
    "df[\"text\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e422dc22-877e-43a7-86b2-332f815cfdbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists: mistral_finetune_data.jsonl. Skipping save.\n"
     ]
    }
   ],
   "source": [
    "# Save final prompt-formatted data as JSONL (skip if already exists)\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "jsonl_path = \"mistral_finetune_data.jsonl\"\n",
    "\n",
    "if os.path.exists(jsonl_path):\n",
    "    print(f\"File already exists: {jsonl_path}. Skipping save.\")\n",
    "else:\n",
    "    with open(jsonl_path, \"w\") as f:\n",
    "        for line in df[\"text\"]:\n",
    "            json.dump({\"text\": line}, f)\n",
    "            f.write(\"\\n\")\n",
    "    print(f\"Dataset saved to {jsonl_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a23900-e2f9-4809-b89d-28298ff7e4ea",
   "metadata": {},
   "source": [
    "### Load Mistral-7B and Apply LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "673c7519-5647-4f1b-a690-a774fa53ea33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4efb198fd2e4311ae649fa7ef18b0a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Mistral-7B base model and tokenizer\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "hf_token   = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    token    = hf_token,\n",
    "    use_fast = True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    token        = hf_token,\n",
    "    device_map   = \"auto\",\n",
    "    torch_dtype  = torch.float16,\n",
    "    load_in_4bit = False,\n",
    "    load_in_8bit = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e7e6356-036f-4c70-a354-95dfb6833b6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 7,251,431,424 || trainable%: 0.0470\n"
     ]
    }
   ],
   "source": [
    "# Apply LoRA (Low-Rank Adaptation) for parameter-efficient fine-tuning\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Define LoRA configuration for causal language modeling\n",
    "lora_config = LoraConfig(\n",
    "    r              = 8,                   # Low-rank dimension\n",
    "    lora_alpha     = 16,                  # Scaling factor for LoRA weights\n",
    "    target_modules = [\"q_proj\", \"v_proj\"],# Target attention projection layers\n",
    "    lora_dropout   = 0.1,                 # Dropout during training\n",
    "    bias           = \"none\",              # Don't apply LoRA to bias terms\n",
    "    task_type      = TaskType.CAUSAL_LM   # Task type is causal language modeling\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print a summary of trainable parameters\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb82a5aa-6253-40f8-9c6b-26f1807f37d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load and tokenize JSONL dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset from JSONL file\n",
    "dataset = load_dataset(\"json\", data_files=\"mistral_finetune_data.jsonl\")\n",
    "\n",
    "# Ensure tokenizer has a pad token (fallback to EOS if missing)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenization function: truncate and pad to max length of 512\n",
    "def tokenize_fn(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "# Apply tokenizer to the dataset and remove the original text\n",
    "tokenized_data = dataset.map(tokenize_fn, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32107680-0ce3-4e99-8e2d-af507b820cea",
   "metadata": {},
   "source": [
    "### Fine-Tuning the Model (LoRA Checkpoints) - DO NOT RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "526c18d0-56ca-4a4e-bde5-ed8a486bd4fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "250d2d9c61ec493fb59cd9b65819f7f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 7,251,431,424 || trainable%: 0.0470\n"
     ]
    }
   ],
   "source": [
    "# Load base model + LoRA wrapper + resume adapter\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "hf_token   = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# Load the base model in float16 with auto device placement\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    token        = hf_token,\n",
    "    torch_dtype  = torch.float16,\n",
    "    device_map   = \"auto\",\n",
    "    load_in_8bit = False,\n",
    "    load_in_4bit = False\n",
    ")\n",
    "\n",
    "# Re-apply LoRA for checkpoint resumption\n",
    "lora_config = LoraConfig(\n",
    "    r             = 8,\n",
    "    lora_alpha    = 16,\n",
    "    target_modules= [\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout  = 0.1,\n",
    "    bias          = \"none\",\n",
    "    task_type     = \"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Wrap the base model with LoRA\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# Load adapter from last saved checkpoint\n",
    "resume_epoch = 3\n",
    "adapter_path = f\"checkpoints_mistral_lora/epoch_{resume_epoch}\"\n",
    "model.load_adapter(adapter_path, adapter_name=\"default\")\n",
    "model.set_adapter(\"default\")  # Activate the loaded adapter\n",
    "\n",
    "# Switch model to training mode\n",
    "model.train()\n",
    "\n",
    "# Print summary of trainable parameters\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "961d84c1-a4fe-418d-ae27-76db333b1908",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6579/1726578809.py:23: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5689' max='5689' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5689/5689 1:22:49, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.188800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.211800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.231600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.236200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.236500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.232200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.234900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.248100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.262900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.256100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.273400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.251600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.275300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.262500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.267400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.268200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>0.268900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.272400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>0.262500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.276900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>0.286500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.276800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>0.274100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.275000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.261900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.265800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>0.267200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.272600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>0.283600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.264700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>0.271200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.252200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>0.271700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.270500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>0.272400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.267100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>0.281200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.263500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>0.258200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.265100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>0.266800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.284200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075</td>\n",
       "      <td>0.273300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.276500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1125</td>\n",
       "      <td>0.287500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.259000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1175</td>\n",
       "      <td>0.249000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.256000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1225</td>\n",
       "      <td>0.284900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.270700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1275</td>\n",
       "      <td>0.257900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.252800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1325</td>\n",
       "      <td>0.255400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.253200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1375</td>\n",
       "      <td>0.253800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.273200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1425</td>\n",
       "      <td>0.252000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.255900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1475</td>\n",
       "      <td>0.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.245900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1525</td>\n",
       "      <td>0.262400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.256500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1575</td>\n",
       "      <td>0.254800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.248400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1625</td>\n",
       "      <td>0.262100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.251300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1675</td>\n",
       "      <td>0.267100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.252700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1725</td>\n",
       "      <td>0.240600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.236200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1775</td>\n",
       "      <td>0.257200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.250900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1825</td>\n",
       "      <td>0.245700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.239900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1875</td>\n",
       "      <td>0.237500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.263400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1925</td>\n",
       "      <td>0.232800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.242600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1975</td>\n",
       "      <td>0.254000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.240800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025</td>\n",
       "      <td>0.244600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.237800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2075</td>\n",
       "      <td>0.225200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.226000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2125</td>\n",
       "      <td>0.220700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.236000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2175</td>\n",
       "      <td>0.233400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.236200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2225</td>\n",
       "      <td>0.218800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2275</td>\n",
       "      <td>0.229700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.243300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2325</td>\n",
       "      <td>0.226700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.245900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2375</td>\n",
       "      <td>0.239400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.234500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2425</td>\n",
       "      <td>0.226300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.236900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2475</td>\n",
       "      <td>0.226500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.230200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2525</td>\n",
       "      <td>0.228000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.238400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2575</td>\n",
       "      <td>0.219400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2625</td>\n",
       "      <td>0.220400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.238300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2675</td>\n",
       "      <td>0.233300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.226700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2725</td>\n",
       "      <td>0.217000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.224600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2775</td>\n",
       "      <td>0.212900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.219300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2825</td>\n",
       "      <td>0.230400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.210700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2875</td>\n",
       "      <td>0.220900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.218900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2925</td>\n",
       "      <td>0.222000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2975</td>\n",
       "      <td>0.214200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.219700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3025</td>\n",
       "      <td>0.192000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.214400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3075</td>\n",
       "      <td>0.196300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.211100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3125</td>\n",
       "      <td>0.203300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.241000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3175</td>\n",
       "      <td>0.212300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.202500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3225</td>\n",
       "      <td>0.219000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.238400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3275</td>\n",
       "      <td>0.209000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.211200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3325</td>\n",
       "      <td>0.213000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.220500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3375</td>\n",
       "      <td>0.207400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.211700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3425</td>\n",
       "      <td>0.212700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.208900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3475</td>\n",
       "      <td>0.195900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.232100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3525</td>\n",
       "      <td>0.205900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.208400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3575</td>\n",
       "      <td>0.199500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.204600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3625</td>\n",
       "      <td>0.204500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.205200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3675</td>\n",
       "      <td>0.201200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.211100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3725</td>\n",
       "      <td>0.193400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>0.199500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3775</td>\n",
       "      <td>0.207700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.206400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3825</td>\n",
       "      <td>0.195400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.203300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3875</td>\n",
       "      <td>0.208700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.203100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3925</td>\n",
       "      <td>0.200900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>0.187900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3975</td>\n",
       "      <td>0.187600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.194500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4025</td>\n",
       "      <td>0.184800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>0.210400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4075</td>\n",
       "      <td>0.191900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.197800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4125</td>\n",
       "      <td>0.193300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>0.202500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4175</td>\n",
       "      <td>0.196400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.206100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4225</td>\n",
       "      <td>0.188500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>0.202200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4275</td>\n",
       "      <td>0.192900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.194300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4325</td>\n",
       "      <td>0.187900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>0.188300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4375</td>\n",
       "      <td>0.203900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.186600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4425</td>\n",
       "      <td>0.192600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>0.181000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4475</td>\n",
       "      <td>0.194800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.182800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4525</td>\n",
       "      <td>0.181400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>0.189800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4575</td>\n",
       "      <td>0.189200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.190700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4625</td>\n",
       "      <td>0.183800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>0.184600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4675</td>\n",
       "      <td>0.185800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.185000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4725</td>\n",
       "      <td>0.184800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>0.191200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4775</td>\n",
       "      <td>0.197500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.179500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4825</td>\n",
       "      <td>0.178500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>0.178900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4875</td>\n",
       "      <td>0.185600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.192000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4925</td>\n",
       "      <td>0.177900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>0.189700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4975</td>\n",
       "      <td>0.178400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.191500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5025</td>\n",
       "      <td>0.182700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>0.184100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5075</td>\n",
       "      <td>0.190500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.177400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5125</td>\n",
       "      <td>0.186700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>0.180500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5175</td>\n",
       "      <td>0.179500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.190900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5225</td>\n",
       "      <td>0.174600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>0.175300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5275</td>\n",
       "      <td>0.172600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.181200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5325</td>\n",
       "      <td>0.182000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>0.193900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5375</td>\n",
       "      <td>0.175400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.197700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5425</td>\n",
       "      <td>0.186900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>0.181000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5475</td>\n",
       "      <td>0.182400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.189300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5525</td>\n",
       "      <td>0.186800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>0.180600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5575</td>\n",
       "      <td>0.187100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.192300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5625</td>\n",
       "      <td>0.192900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>0.188400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5675</td>\n",
       "      <td>0.188700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chockalingamveerap.n/.local/lib/python3.9/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error 401 Client Error: Unauthorized for url: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3/resolve/main/config.json (Request ID: Root=1-67f65344-4006aa240a9957382764e1f0;407fbd9e-f77c-457d-89b5-8f1b710a426e)\n",
      "\n",
      "Invalid credentials in Authorization header - silently ignoring the lookup for the file config.json in mistralai/Mistral-7B-Instruct-v0.3.\n",
      "  warnings.warn(\n",
      "/home/chockalingamveerap.n/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in mistralai/Mistral-7B-Instruct-v0.3 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/home/chockalingamveerap.n/.local/lib/python3.9/site-packages/peft/utils/other.py:1107: UserWarning: Unable to fetch remote file due to the following error 401 Client Error: Unauthorized for url: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3/resolve/main/config.json (Request ID: Root=1-67f65344-2d41504a624c87385e6f02ff;6e13401c-a81e-4ea4-8e9f-68a1d64bfd1e)\n",
      "\n",
      "Invalid credentials in Authorization header - silently ignoring the lookup for the file config.json in mistralai/Mistral-7B-Instruct-v0.3.\n",
      "  warnings.warn(\n",
      "/home/chockalingamveerap.n/.local/lib/python3.9/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in mistralai/Mistral-7B-Instruct-v0.3 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('checkpoints_mistral_lora/epoch_4/tokenizer_config.json',\n",
       " 'checkpoints_mistral_lora/epoch_4/special_tokens_map.json',\n",
       " 'checkpoints_mistral_lora/epoch_4/tokenizer.model',\n",
       " 'checkpoints_mistral_lora/epoch_4/added_tokens.json',\n",
       " 'checkpoints_mistral_lora/epoch_4/tokenizer.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train for the next epoch\n",
    "\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "epoch_num  = 4  # Set the next epoch number manually\n",
    "output_dir = f\"checkpoints_mistral_lora/epoch_{epoch_num}\"  # Where to save model and tokenizer\n",
    "\n",
    "# Define training hyperparameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir                  = output_dir,\n",
    "    per_device_train_batch_size = 2,\n",
    "    gradient_accumulation_steps = 4,     # Effective batch size = 2 * 4 = 8\n",
    "    num_train_epochs            = 1,     # Only 1 epoch per burst session\n",
    "    learning_rate               = 2e-4,\n",
    "    fp16                        = True,  # Use mixed precision\n",
    "    save_strategy               =\"epoch\",# Save model at the end of each epoch\n",
    "    save_total_limit            = 2,     # Keep only the 2 most recent checkpoints\n",
    "    logging_steps               = 25,\n",
    "    report_to                   = \"none\" # Disable logging to WandB/HuggingFace\n",
    ")\n",
    "\n",
    "# Create a data collator for causal language modeling (no MLM)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "# Set up Trainer\n",
    "trainer = Trainer(\n",
    "    model         = model,\n",
    "    args          = training_args,\n",
    "    train_dataset = tokenized_data[\"train\"],\n",
    "    tokenizer     = tokenizer,\n",
    "    data_collator = data_collator\n",
    ")\n",
    "\n",
    "# Run training for one epoch\n",
    "trainer.train()\n",
    "\n",
    "# Save model and tokenizer to disk\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b055895f-9d99-47fc-8c9a-f2379e5a40ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Define safe temp and final zip paths\n",
    "temp_zip_path = \"checkpoints_mistral_lora.zip\"  # Local working directory (you have access)\n",
    "final_zip_path = \"checkpoints_mistral_lora.zip\"\n",
    "\n",
    "# Remove existing if needed\n",
    "if os.path.exists(temp_zip_path):\n",
    "    os.remove(temp_zip_path)\n",
    "if os.path.exists(final_zip_path):\n",
    "    os.remove(final_zip_path)\n",
    "\n",
    "# Create the zip in current working directory\n",
    "shutil.make_archive(\"checkpoints_mistral_lora\", 'zip', \"checkpoints_mistral_lora\")\n",
    "\n",
    "# Move it to /mnt/data for download\n",
    "shutil.move(temp_zip_path, final_zip_path)\n",
    "\n",
    "print(\"✅ Zipped and moved successfully. Ready to download:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b2429f-73df-47a3-866b-5d34837e5343",
   "metadata": {},
   "source": [
    "### Section 6: Load Adapter and Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d419a3bb-3f53-4d11-b628-c7daa2f72c91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MistralForCausalLM(\n",
       "      (model): MistralModel(\n",
       "        (embed_tokens): Embedding(32768, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x MistralDecoderLayer(\n",
       "            (self_attn): MistralAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): MistralMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): MistralRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load adapter only — assumes model and tokenizer already loaded in Section 4\n",
    "\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load LoRA adapter from latest completed epoch\n",
    "last_epoch   = 3\n",
    "adapter_path = f\"checkpoints_mistral_lora/epoch_{last_epoch}\"\n",
    "\n",
    "model.load_adapter(adapter_path, adapter_name=\"default\")   # Load adapter weights\n",
    "model.set_adapter(\"default\")                               # Activate the adapter\n",
    "model.eval()                                               # Set model to eval mode for inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "871f8489-37f6-4450-b43b-2eb08dbab97e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prompt 1 ---\n",
      "\n",
      "philosophers — people who engage in deep thought about important questions and issues related to existence, knowledge, ethics, etc.\n",
      "\n",
      "--- Prompt 2 ---\n",
      "\n",
      "bioluminescent — emitting light produced by living organisms or cells, especially through chemical reactions involving oxygen and energy from metabolism.\n",
      "\n",
      "--- Prompt 3 ---\n",
      "\n",
      "catastrophic — causing great destruction or loss; disastrous — severe flooding, etc.\n",
      "\n",
      "--- Prompt 4 ---\n",
      "\n",
      "unwavering — not changing or wavering; firm, steady, constant — Dictionary.com\n"
     ]
    }
   ],
   "source": [
    "# Sample passages to test inference quality\n",
    "\n",
    "test_prompts = [\n",
    "    \"\"\"Text: The philosopher pondered the metaphysical implications of the paradox, which perplexed even the most seasoned scholars.\n",
    "Q: Identify all the words in the passage that difficult for a highschooler. Return them as a comma-separated list with their meaning.\n",
    "A:\"\"\",\n",
    "    \n",
    "    \"\"\"Text: The bioluminescent plankton illuminated the waves, mesmerizing the tourists who had never witnessed such a natural phenomenon.\n",
    "Q: Identify all the words in the passage that difficult for a highschooler. Return them as a comma-separated list with their meaning.\n",
    "\n",
    "A:\"\"\",\n",
    "    \n",
    "    \"\"\"Text: Despite the catastrophic hurricane, the city's infrastructure withstood the damage due to meticulous planning and resilient materials.\n",
    "Q: Identify all the words in the passage that difficult for a highschooler. Return them as a comma-separated list with their meaning.\n",
    "\n",
    "A:\"\"\",\n",
    "    \n",
    "    \"\"\"Text: Her altruistic actions and unwavering resolve inspired a new generation of reformers.\n",
    "Q: Identify all the words in the passage that difficult for a highschooler. Return them as a comma-separated list with their meaning.\n",
    "\n",
    "A:\"\"\"\n",
    "]\n",
    "\n",
    "# Run inference with cleaned output\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    print(f\"\\n--- Prompt {i + 1} ---\\n\")\n",
    "\n",
    "    # Tokenize prompt and move to model's device\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens     = 120,\n",
    "            temperature        = 0.3,    # Low temperature for deterministic output\n",
    "            do_sample          = True,\n",
    "            repetition_penalty = 1.2,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract clean A: block and ignore repeated Q:/A:\n",
    "    if \"A:\" in decoded:\n",
    "        answer_section = decoded.split(\"A:\", 1)[1]\n",
    "        answer_section = answer_section.split(\"Q:\", 1)[0].strip()\n",
    "    else:\n",
    "        answer_section = decoded.strip()\n",
    "\n",
    "    print(answer_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f0a85ba-d87d-421d-b9d7-72045d209fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ [EPOCH3 OUTPUT] ================\n",
      "\n",
      "\n",
      "--- Prompt 1 ---\n",
      "scholars — people who have studied a subject for a long time and are experts in it; academics or intellectuals.\n",
      "\n",
      "--- Prompt 2 ---\n",
      "bioluminescent — emitting light produced by living organisms or cells due to chemical reactions within them.\n",
      "plankton — microscopic plants and animals that drift through water columns in oceans and lakes.\n",
      "phenomena — events, things, or circumstances observed; occurrences.\n",
      "tourists — people traveling for pleasure rather than work.\n",
      "witnesses — persons present at an event or situation.\n",
      "\n",
      "--- Prompt 3 ---\n",
      "catastrophic — causing great destruction or loss; disastrous — severe hurricane that caused significant damage.\n",
      "\n",
      "--- Prompt 4 ---\n",
      "resolution — firm decision or determination; strong belief or conviction about something; ability to solve problems effectively or make things happen successfully.\n",
      "\n",
      "================ [EPOCH4 OUTPUT] ================\n",
      "\n",
      "\n",
      "--- Prompt 1 ---\n",
      "scholars — people who have studied a subject for many years and are experts in it; academics or intellectuals.\n",
      "\n",
      "--- Prompt 2 ---\n",
      "bioluminescent — emitting light produced by living organisms or substances that can be seen from outside the body of an animal or plant.\n",
      "emitting — giving out; sending out something, especially energy or radiation.\n",
      "plankton — microscopic plants and animals floating or drifting in water bodies.\n",
      "\n",
      "--- Prompt 3 ---\n",
      "catastrophic — causing great destruction or loss; disastrous; calamitous — causing widespread devastation or ruin; extremely serious or harmful; a disaster of major proportions.\n",
      "\n",
      "--- Prompt 4 ---\n",
      "resolution — determination; firmness of purpose or intention to act decisively on a matter despite obstacles, difficulties, or opposition.\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "# Define test prompts\n",
    "test_prompts = [\n",
    "    \"Text: The philosopher pondered the metaphysical implications of the paradox, which perplexed even the most seasoned scholars.\\nQ: List all difficult words in the sentence along with their definitions. Format as: word — definition.\\nA:\",\n",
    "    \"Text: The bioluminescent plankton illuminated the waves, mesmerizing the tourists who had never witnessed such a natural phenomenon.\\nQ: List all difficult words in the sentence along with their definitions. Format as: word — definition.\\nA:\",\n",
    "    \"Text: Despite the catastrophic hurricane, the city's infrastructure withstood the damage due to meticulous planning and resilient materials.\\nQ: List all difficult words in the sentence along with their definitions. Format as: word — definition.\\nA:\",\n",
    "    \"Text: Her altruistic actions and unwavering resolve inspired a new generation of reformers.\\nQ: List all difficult words in the sentence along with their definitions. Format as: word — definition.\\nA:\"\n",
    "]\n",
    "\n",
    "# Function to run inference and extract clean output\n",
    "def run_with_adapter(adapter_path, label):\n",
    "    # Load adapter\n",
    "    model.load_adapter(adapter_path, adapter_name=label)\n",
    "    model.set_adapter(label)\n",
    "    model.eval()\n",
    "\n",
    "    print(f\"\\n================ [{label.upper()} OUTPUT] ================\\n\")\n",
    "    for i, prompt in enumerate(test_prompts):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                **inputs,\n",
    "                do_sample      = False,\n",
    "                pad_token_id   = tokenizer.eos_token_id,\n",
    "                max_new_tokens = 120,\n",
    "                repetition_penalty = 1.2,\n",
    "            )\n",
    "\n",
    "\n",
    "        decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "        # Clean response: get only what's after first A:, and stop before repeated Q:/A:\n",
    "        answer = decoded.split(\"A:\", 1)[-1].split(\"Q:\")[0].strip()\n",
    "\n",
    "        # Remove labels like \"B:\" or \"C:\" at start of lines\n",
    "        answer = re.sub(r'^[A-Z]:\\s*', '', answer, flags=re.MULTILINE)\n",
    "\n",
    "        print(f\"\\n--- Prompt {i + 1} ---\")\n",
    "        print(answer)\n",
    "\n",
    "# Run comparisons\n",
    "run_with_adapter(\"checkpoints_mistral_lora/epoch_3\", \"epoch3\")\n",
    "run_with_adapter(\"checkpoints_mistral_lora/epoch_4\", \"epoch4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e624d63-c91b-4720-b422-a52d42490ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique passages for evaluation: 2412\n"
     ]
    }
   ],
   "source": [
    "# Group Ground Truth Annotations \n",
    "import pandas as pd\n",
    "\n",
    "# Load the curated dataset\n",
    "eval_df = pd.read_csv(\"curated_dataset.csv\")\n",
    "eval_df.dropna(subset=[\"passage\", \"word\"], inplace=True)\n",
    "\n",
    "# Group by passage to get a set of ground truth difficult words per passage\n",
    "gt_grouped = eval_df.groupby(\"passage\")[\"word\"].apply(lambda words: set(w.strip().lower() for w in words)).reset_index()\n",
    "print(\"Number of unique passages for evaluation:\", len(gt_grouped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26b64d52-95e7-4c88-91dc-ba6da420fe61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed words: {'infrastructure', 'resilient', 'catastrophic', 'ruin', 'disastrous'}\n",
      "Parsed words: {'truth', 'knowledge', 'l: philosophers', 'm: engaged', 'effects', 'etc.', 's: scholars', 'implications'}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def parse_output(output_text):\n",
    "    \"\"\"\n",
    "    Parses model output to extract only the difficult words (lowercased).\n",
    "    Expected format: one or more lines, each like \"word — definition\".\n",
    "    Handles extra formatting and avoids pulling in definitions.\n",
    "    \"\"\"\n",
    "    annotations = set()\n",
    "\n",
    "    # Split by newlines, semicolons, or commas\n",
    "    segments = re.split(r'[\\n;,]', output_text.strip())\n",
    "\n",
    "    for segment in segments:\n",
    "        segment = segment.strip()\n",
    "\n",
    "        # Skip empty lines\n",
    "        if not segment:\n",
    "            continue\n",
    "\n",
    "        # If it looks like \"word — definition\"\n",
    "        if '—' in segment:\n",
    "            word = segment.split('—', 1)[0].strip().lower()\n",
    "            if word:\n",
    "                annotations.add(word)\n",
    "\n",
    "        # Fallback: single word\n",
    "        elif len(segment.split()) == 1:\n",
    "            annotations.add(segment.lower())\n",
    "\n",
    "    return annotations\n",
    "\n",
    "# Test Case\n",
    "test_text = \"catastrophic — causing great and sudden damage or ruin; disastrous — causing great and sudden damage or ruin; disastrous — causing great and sudden damage or ruin; ruin — the destruction or loss of something valuable or important; resilient — able to withstand damage or criticism and recover quickly; infrastructure — the basic physical and organizational structures and services needed for a city or community to function.\"\n",
    "print(\"Parsed words:\", parse_output(test_text))\n",
    "\n",
    "test_text = str(\"implications — consequences or results that follow from something else; effects \\n L: philosophers — people who engage in deep and abstract thinking about questions related to existence, knowledge, truth, good and beauty, etc.\\n M: engaged — involved or occupied with something\\n S: scholars — learned persons; experts in a particular field\")\n",
    "print(\"Parsed words:\", parse_output(test_text))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93bfa61f-8f42-4452-a7f5-233f7a4ec7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#Define lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def normalize(word):\n",
    "    return lemmatizer.lemmatize(word.lower().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "187c77d2-36a1-444f-a365-372151b6fa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model Prediction on a Single Passage\n",
    "def evaluate_passage(passage, ground_truth_words, max_new_tokens=120):\n",
    "    \"\"\"\n",
    "    Run model inference on the given passage prompt and compare with ground truth difficult words.\n",
    "    Returns a dictionary with predicted words, ground truth, and computed precision, recall, and F1.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        f\"Text: {passage}\\n\"\n",
    "        \"Q: List all difficult words in the sentence along with their definitions. \"\n",
    "        \"Format as: word — definition.\\nA:\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.2,\n",
    "            max_new_tokens=max_new_tokens\n",
    "        )\n",
    "\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract answer section (everything after first \"A:\")\n",
    "    answer_section = decoded.split(\"A:\", 1)[-1].split(\"Q:\")[0].strip()\n",
    "\n",
    "    # Post-process: remove B:, C:, etc.\n",
    "    answer_section = re.sub(r'^[A-Z]:\\s*', '', answer_section, flags=re.MULTILINE)\n",
    "\n",
    "    # Parse predictions\n",
    "    predicted_words_raw = parse_output(answer_section)\n",
    "\n",
    "    # Normalize both predicted and ground truth words\n",
    "    predicted_words = set(normalize(w) for w in predicted_words_raw)\n",
    "    ground_truth    = set(normalize(w) for w in ground_truth_words)\n",
    "\n",
    "    # Calculate metrics\n",
    "    tp = len(predicted_words & ground_truth)\n",
    "    fp = len(predicted_words - ground_truth)\n",
    "    fn = len(ground_truth - predicted_words)\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    recall    = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    f1        = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        \"passage\"         : passage,\n",
    "        \"predicted_words\" : predicted_words_raw,  # keep original for inspection\n",
    "        \"ground_truth\"    : ground_truth_words,\n",
    "        \"precision\"       : precision,\n",
    "        \"recall\"          : recall,\n",
    "        \"f1\"              : f1,\n",
    "        \"raw_output\"      : answer_section\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2b7085c-e990-4e95-adff-d6cd144bd842",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/2412 [00:06<4:15:58,  6.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passage Evaluation:\n",
      "Ground Truth Words: {'lobet', 'liebstes', 'alto', 'ach', 'cantata', 'bwv', 'reichen', 'agnus dei', 'gewalt', 'obbligato', 'aria', 'unison', 'ascension', 'oratorio'}\n",
      "Predicted Words: {'elevation.', 'ascension', 'altos', 'cantata', 'hymn', 'agnus dei'}\n",
      "Precision: 0.67, Recall: 0.29, F1: 0.40\n",
      "Raw model output:\n",
      " ascension — the act of rising to a higher position or level; elevation.\n",
      "agnus dei — Latin phrase meaning 'lamb of God'; a hymn or prayer invoking Jesus Christ's mercy and compassion.\n",
      "altos — female singers who have lower voices than sopranos but higher than contraltos.\n",
      "cantata — a musical composition typically consisting of several movements, usually for solo voice(s) and instrumental accompaniment.\n",
      "hymn — a religious song expressing praise or devotion to God.\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/2412 [00:07<2:01:54,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passage Evaluation:\n",
      "Ground Truth Words: {'episode', 'originally', 'aired', 'guest stars', 'comedy', 'office', 'linage', 'actor', 'ancestry', 'friend', 'steve', 'television series', 'directed', 'actor friend'}\n",
      "Predicted Words: {'original air date'}\n",
      "Precision: 0.00, Recall: 0.00, F1: 0.00\n",
      "Raw model output:\n",
      " original air date — the first time an episode or program was broadcast to the public.\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/2412 [00:10<2:14:06,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passage Evaluation:\n",
      "Ground Truth Words: {'sentimental', 'fulfilled', 'refined', 'texture', 'synth', 'intro', 'chauvinism', 'borderline', 'instrumentations', 'doubled'}\n",
      "Predicted Words: {'vocalist', 'textures', 'instrumentation', 'instrumental', 'synthesizer'}\n",
      "Precision: 0.40, Recall: 0.20, F1: 0.27\n",
      "Raw model output:\n",
      " instrumental — relating to or using instruments rather than voices for musical expression; not sung but played on an instrument.\n",
      "synthesizer — electronic musical instrument capable of generating sounds from digital signals, typically producing music without human performers.\n",
      "textures — the feel or appearance of a surface or material.\n",
      "vocalist — a person who sings songs or performs vocals.\n",
      "instrumentation — the use of instruments in music or other artistic works.\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/2412 [00:14<2:24:20,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passage Evaluation:\n",
      "Ground Truth Words: {'parlays', 'moves', 'peace', '1994', 'parleys', '@-@', 'presse', 'serb', 'deutsche', 'december', 'agentur'}\n",
      "Predicted Words: {'divisions', 'district', 'delegates', 'parliamentary', 'conference'}\n",
      "Precision: 0.00, Recall: 0.00, F1: 0.00\n",
      "Raw model output:\n",
      " parliamentary — relating to a legislative body or government by representatives elected from districts or political parties.\n",
      "delegates — people who are chosen to represent others, especially at meetings or conferences.\n",
      "district — an area defined for administrative purposes; a division of a country or city.\n",
      "conference — a meeting where important decisions are made about something, typically involving many different organizations or groups.\n",
      "divisions — a group within a larger organization that is responsible for specific tasks or functions.\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/2412 [00:16<1:55:06,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passage Evaluation:\n",
      "Ground Truth Words: {'topnotch', 'need', 'boy', 'tox', 'mix'}\n",
      "Predicted Words: {'topnotch', 'first-rate or best-quality'}\n",
      "Precision: 0.50, Recall: 0.20, F1: 0.29\n",
      "Raw model output:\n",
      " topnotch — of high quality or excellence; superior or outstanding; first-rate or best-quality — used as an adjective to describe something or someone that is very good or excellent.\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2412/2412 [1:41:57<00:00,  2.54s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision: 0.45\n",
      "Average Recall: 0.15\n",
      "Average F1 Score: 0.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run Evaluation on Sample Passages\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Adjust the number of samples as needed\n",
    "# num_samples = 5  \n",
    "# sampled     = gt_grouped.sample(n=num_samples, random_state=42)\n",
    "count       = 0\n",
    "results     = []\n",
    "\n",
    "for _, row in tqdm(gt_grouped.iterrows(), total=len(gt_grouped)):\n",
    "    passage  = row[\"passage\"]\n",
    "    gt_words = row[\"word\"]\n",
    "    # Depending on how the grouping is saved in CSV, gt_words might be a string.\n",
    "    if isinstance(gt_words, str):\n",
    "        # If it appears as a comma-separated string, convert it to a set.\n",
    "        gt_words_set = set(w.strip().lower() for w in gt_words.split(\",\") if w.strip())\n",
    "    else:\n",
    "        gt_words_set = gt_words\n",
    "\n",
    "    res = evaluate_passage(passage, gt_words_set)\n",
    "    results.append(res)\n",
    "    \n",
    "    #Print first 5 outputs for reference\n",
    "    if(count<5):\n",
    "        print(\"Passage Evaluation:\")\n",
    "        print(\"Ground Truth Words:\", gt_words_set)\n",
    "        print(\"Predicted Words:\", res[\"predicted_words\"])\n",
    "        print(f\"Precision: {res['precision']:.2f}, Recall: {res['recall']:.2f}, F1: {res['f1']:.2f}\")\n",
    "        print(\"Raw model output:\\n\", res[\"raw_output\"])\n",
    "        print(\"-\" * 60)\n",
    "    count += 1\n",
    "\n",
    "# Calculate and print average metrics\n",
    "avg_precision = sum(r[\"precision\"] for r in results) / len(results)\n",
    "avg_recall    = sum(r[\"recall\"] for r in results) / len(results)\n",
    "avg_f1        = sum(r[\"f1\"] for r in results) / len(results)\n",
    "\n",
    "print(f\"Average Precision: {avg_precision:.2f}\")\n",
    "print(f\"Average Recall: {avg_recall:.2f}\")\n",
    "print(f\"Average F1 Score: {avg_f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "714062f8-3041-4d3c-98e0-c5388d12e9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision: 0.45\n",
      "Average Recall:    0.15\n",
      "Average F1 Score:  0.21\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Consolidated Metrics and Histograms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract metric scores from evaluation results (the 'results' list produced in your previous evaluation cell)\n",
    "precision_scores = [r['precision'] for r in results]\n",
    "recall_scores    = [r['recall'] for r in results]\n",
    "f1_scores        = [r['f1'] for r in results]\n",
    "\n",
    "# Print overall averages\n",
    "avg_precision = sum(precision_scores) / len(precision_scores)\n",
    "avg_recall    = sum(recall_scores) / len(recall_scores)\n",
    "avg_f1        = sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "print(f\"Average Precision: {avg_precision:.2f}\")\n",
    "print(f\"Average Recall:    {avg_recall:.2f}\")\n",
    "print(f\"Average F1 Score:  {avg_f1:.2f}\")\n",
    "\n",
    "# Plot distributions\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.hist(precision_scores, bins=20, color='blue', alpha=0.7)\n",
    "plt.title(\"Precision Distribution\")\n",
    "plt.xlabel(\"Precision\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.hist(recall_scores, bins=20, color='green', alpha=0.7)\n",
    "plt.title(\"Recall Distribution\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.hist(f1_scores, bins=20, color='red', alpha=0.7)\n",
    "plt.title(\"F1 Score Distribution\")\n",
    "plt.xlabel(\"F1 Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b6761b8-3ff7-48d3-826e-8eda289014f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best/worst examples to f1_extremes_summary.txt ✅\n"
     ]
    }
   ],
   "source": [
    "with open(\"f1_extremes_summary.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"=== Worst 3 Passages (by F1) ===\\n\")\n",
    "    for r in sorted(results, key=lambda x: x['f1'])[:3]:\n",
    "        f.write(f\"\\nPassage: {r['passage']}\\n\")\n",
    "        f.write(f\"GT: {r['ground_truth']}\\n\")\n",
    "        f.write(f\"Pred: {r['predicted_words']}\\n\")\n",
    "        f.write(f\"F1: {r['f1']:.2f}\\n\")\n",
    "\n",
    "    f.write(\"\\n=== Best 3 Passages (by F1) ===\\n\")\n",
    "    for r in sorted(results, key=lambda x: x['f1'], reverse=True)[:3]:\n",
    "        f.write(f\"\\nPassage: {r['passage']}\\n\")\n",
    "        f.write(f\"GT: {r['ground_truth']}\\n\")\n",
    "        f.write(f\"Pred: {r['predicted_words']}\\n\")\n",
    "        f.write(f\"F1: {r['f1']:.2f}\\n\")\n",
    "\n",
    "print(\"Saved best/worst examples to f1_extremes_summary.txt ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "474cc81b-8e5f-458c-8a2a-6d3cd6c6a1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved evaluation with average row at bottom.\n",
      "Evaluation results have been saved to evaluation_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_98695/386772506.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  eval_results_df = eval_results_df.append(summary_row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Save Evaluation Results\n",
    "import pandas as pd\n",
    "\n",
    "eval_results_df = pd.DataFrame(results)\n",
    "eval_results_df.to_csv(\"evaluation_results.csv\", index=False)\n",
    "# Add summary row to CSV\n",
    "summary_row = {\n",
    "    \"passage\": \"AVERAGE\",\n",
    "    \"predicted_words\": \"\",\n",
    "    \"ground_truth\": \"\",\n",
    "    \"precision\": avg_precision,\n",
    "    \"recall\": avg_recall,\n",
    "    \"f1\": avg_f1,\n",
    "    \"raw_output\": \"\"\n",
    "}\n",
    "eval_results_df = eval_results_df.append(summary_row, ignore_index=True)\n",
    "eval_results_df.to_csv(\"evaluation_results.csv\", index=False)\n",
    "print(\"Saved evaluation with average row at bottom.\")\n",
    "print(\"Evaluation results have been saved to evaluation_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a887b131-ff46-47b6-a127-6722aab67294",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
